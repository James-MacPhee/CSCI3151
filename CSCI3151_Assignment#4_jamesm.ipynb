{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq</th>\n",
       "      <th>PMI</th>\n",
       "      <th>t-test</th>\n",
       "      <th>Chi-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((Subject, NNP), (Re, NNP))</td>\n",
       "      <td>((Evelyn, NNP), (Conlon, NNP))</td>\n",
       "      <td>((Subject, NNP), (Re, NNP))</td>\n",
       "      <td>((ALink, NNP), (KSAND, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((Organization, NNP), (University, NNP))</td>\n",
       "      <td>((Duck, NNP), (Pond, NNP))</td>\n",
       "      <td>((Organization, NNP), (University, NNP))</td>\n",
       "      <td>((Carnegie, NNP), (Mellon, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((Lines, NNP), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))</td>\n",
       "      <td>((Lines, NNP), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((Cookamunga, NNP), (Tourist, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((Distribution, NNP), (world, NN))</td>\n",
       "      <td>((ancient, NN), (Mayans, NNPS))</td>\n",
       "      <td>((Distribution, NNP), (world, NN))</td>\n",
       "      <td>((Evelyn, NNP), (Conlon, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((Lines, NNP), (Distribution, NNP))</td>\n",
       "      <td>((Notre, NNP), (Dame, NNP))</td>\n",
       "      <td>((Lines, NNP), (Distribution, NNP))</td>\n",
       "      <td>((Notre, NNP), (Dame, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((world, NN), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((Eau, NNP), (Claire, NNP))</td>\n",
       "      <td>((world, NN), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((OriginalSender, NNP), (isuVACATIONVENARICSCM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((Henry, NNP), (Spencer, NNP))</td>\n",
       "      <td>((Tape, NNP), (Cites, NNP))</td>\n",
       "      <td>((Henry, NNP), (Spencer, NNP))</td>\n",
       "      <td>((fait, NN), (comme, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((Computer, NNP), (Science, NNP))</td>\n",
       "      <td>((Frequently, NNP), (Asked, NNP))</td>\n",
       "      <td>((Computer, NNP), (Science, NNP))</td>\n",
       "      <td>((Eau, NNP), (Claire, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((TIN, NNP), (version, NN))</td>\n",
       "      <td>((Southwestern, NNP), (Louisiana, NNP))</td>\n",
       "      <td>((TIN, NNP), (version, NN))</td>\n",
       "      <td>((Duck, NNP), (Pond, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((version, NN), (PL, NNP))</td>\n",
       "      <td>((fait, NN), (comme, NN))</td>\n",
       "      <td>((version, NN), (PL, NNP))</td>\n",
       "      <td>((Mantis, NNP), (Consultants, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((XNewsreader, NNP), (TIN, NNP))</td>\n",
       "      <td>((sank, JJ), (Manhattan, NNP))</td>\n",
       "      <td>((XNewsreader, NNP), (TIN, NNP))</td>\n",
       "      <td>((Los, NNP), (Angeles, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((Jon, NNP), (Livesey, NNP))</td>\n",
       "      <td>((GENERAL, NNP), (UNIFIED, NNP))</td>\n",
       "      <td>((Jon, NNP), (Livesey, NNP))</td>\n",
       "      <td>((VAXVMS, NNP), (VNEWS, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((Does, NNP), (anyone, NN))</td>\n",
       "      <td>((Steinn, NNP), (Sigurdsson, NNP))</td>\n",
       "      <td>((Does, NNP), (anyone, NN))</td>\n",
       "      <td>((sank, JJ), (Manhattan, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((Apr, NNP), (GMT, NNP))</td>\n",
       "      <td>((Beam, NNP), (Jockey, NNP))</td>\n",
       "      <td>((Apr, NNP), (GMT, NNP))</td>\n",
       "      <td>((ISLAMIC, NNP), (LAW, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((Kent, NNP), (Sandvik, NNP))</td>\n",
       "      <td>((icsucieduincominggeodegif, NN), (icsucieduin...</td>\n",
       "      <td>((Kent, NNP), (Sandvik, NNP))</td>\n",
       "      <td>((Steinn, NNP), (Sigurdsson, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((State, NNP), (University, NNP))</td>\n",
       "      <td>((bzawutarlgutaedu, NN), (stephen, NN))</td>\n",
       "      <td>((State, NNP), (University, NNP))</td>\n",
       "      <td>((Beam, NNP), (Jockey, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((PL, NNP), (Lines, NNP))</td>\n",
       "      <td>((Asked, NNP), (Questions, NNP))</td>\n",
       "      <td>((PL, NNP), (Lines, NNP))</td>\n",
       "      <td>((icsucieduincominggeodegif, NN), (icsucieduin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((Frank, NNP), (ODwyer, NNP))</td>\n",
       "      <td>((MY, NNP), (REPLY, NNP))</td>\n",
       "      <td>((Frank, NNP), (ODwyer, NNP))</td>\n",
       "      <td>((GENERAL, NNP), (UNIFIED, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((Toronto, NNP), (Zoology, NNP))</td>\n",
       "      <td>((Dryden, NNP), (FredMcCalldsegticom, NNP))</td>\n",
       "      <td>((Toronto, NNP), (Zoology, NNP))</td>\n",
       "      <td>((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((University, NNP), (Lines, NNP))</td>\n",
       "      <td>((ISLAMIC, NNP), (LAW, NNP))</td>\n",
       "      <td>((Jesus, NNP), (Christ, NNP))</td>\n",
       "      <td>((Tourist, NNP), (Bureau, NNP))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Freq  \\\n",
       "0                ((Subject, NNP), (Re, NNP))   \n",
       "1   ((Organization, NNP), (University, NNP))   \n",
       "2     ((Lines, NNP), (NNTPPostingHost, NNP))   \n",
       "3         ((Distribution, NNP), (world, NN))   \n",
       "4        ((Lines, NNP), (Distribution, NNP))   \n",
       "5      ((world, NN), (NNTPPostingHost, NNP))   \n",
       "6             ((Henry, NNP), (Spencer, NNP))   \n",
       "7          ((Computer, NNP), (Science, NNP))   \n",
       "8                ((TIN, NNP), (version, NN))   \n",
       "9                 ((version, NN), (PL, NNP))   \n",
       "10          ((XNewsreader, NNP), (TIN, NNP))   \n",
       "11              ((Jon, NNP), (Livesey, NNP))   \n",
       "12               ((Does, NNP), (anyone, NN))   \n",
       "13                  ((Apr, NNP), (GMT, NNP))   \n",
       "14             ((Kent, NNP), (Sandvik, NNP))   \n",
       "15         ((State, NNP), (University, NNP))   \n",
       "16                 ((PL, NNP), (Lines, NNP))   \n",
       "17             ((Frank, NNP), (ODwyer, NNP))   \n",
       "18          ((Toronto, NNP), (Zoology, NNP))   \n",
       "19         ((University, NNP), (Lines, NNP))   \n",
       "\n",
       "                                                  PMI  \\\n",
       "0                      ((Evelyn, NNP), (Conlon, NNP))   \n",
       "1                          ((Duck, NNP), (Pond, NNP))   \n",
       "2   ((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))   \n",
       "3                     ((ancient, NN), (Mayans, NNPS))   \n",
       "4                         ((Notre, NNP), (Dame, NNP))   \n",
       "5                         ((Eau, NNP), (Claire, NNP))   \n",
       "6                         ((Tape, NNP), (Cites, NNP))   \n",
       "7                   ((Frequently, NNP), (Asked, NNP))   \n",
       "8             ((Southwestern, NNP), (Louisiana, NNP))   \n",
       "9                           ((fait, NN), (comme, NN))   \n",
       "10                     ((sank, JJ), (Manhattan, NNP))   \n",
       "11                   ((GENERAL, NNP), (UNIFIED, NNP))   \n",
       "12                 ((Steinn, NNP), (Sigurdsson, NNP))   \n",
       "13                       ((Beam, NNP), (Jockey, NNP))   \n",
       "14  ((icsucieduincominggeodegif, NN), (icsucieduin...   \n",
       "15            ((bzawutarlgutaedu, NN), (stephen, NN))   \n",
       "16                   ((Asked, NNP), (Questions, NNP))   \n",
       "17                          ((MY, NNP), (REPLY, NNP))   \n",
       "18        ((Dryden, NNP), (FredMcCalldsegticom, NNP))   \n",
       "19                       ((ISLAMIC, NNP), (LAW, NNP))   \n",
       "\n",
       "                                      t-test  \\\n",
       "0                ((Subject, NNP), (Re, NNP))   \n",
       "1   ((Organization, NNP), (University, NNP))   \n",
       "2     ((Lines, NNP), (NNTPPostingHost, NNP))   \n",
       "3         ((Distribution, NNP), (world, NN))   \n",
       "4        ((Lines, NNP), (Distribution, NNP))   \n",
       "5      ((world, NN), (NNTPPostingHost, NNP))   \n",
       "6             ((Henry, NNP), (Spencer, NNP))   \n",
       "7          ((Computer, NNP), (Science, NNP))   \n",
       "8                ((TIN, NNP), (version, NN))   \n",
       "9                 ((version, NN), (PL, NNP))   \n",
       "10          ((XNewsreader, NNP), (TIN, NNP))   \n",
       "11              ((Jon, NNP), (Livesey, NNP))   \n",
       "12               ((Does, NNP), (anyone, NN))   \n",
       "13                  ((Apr, NNP), (GMT, NNP))   \n",
       "14             ((Kent, NNP), (Sandvik, NNP))   \n",
       "15         ((State, NNP), (University, NNP))   \n",
       "16                 ((PL, NNP), (Lines, NNP))   \n",
       "17             ((Frank, NNP), (ODwyer, NNP))   \n",
       "18          ((Toronto, NNP), (Zoology, NNP))   \n",
       "19             ((Jesus, NNP), (Christ, NNP))   \n",
       "\n",
       "                                          Chi-squared  \n",
       "0                        ((ALink, NNP), (KSAND, NNP))  \n",
       "1                    ((Carnegie, NNP), (Mellon, NNP))  \n",
       "2                 ((Cookamunga, NNP), (Tourist, NNP))  \n",
       "3                      ((Evelyn, NNP), (Conlon, NNP))  \n",
       "4                         ((Notre, NNP), (Dame, NNP))  \n",
       "5   ((OriginalSender, NNP), (isuVACATIONVENARICSCM...  \n",
       "6                           ((fait, NN), (comme, NN))  \n",
       "7                         ((Eau, NNP), (Claire, NNP))  \n",
       "8                          ((Duck, NNP), (Pond, NNP))  \n",
       "9                 ((Mantis, NNP), (Consultants, NNP))  \n",
       "10                       ((Los, NNP), (Angeles, NNP))  \n",
       "11                      ((VAXVMS, NNP), (VNEWS, NNP))  \n",
       "12                     ((sank, JJ), (Manhattan, NNP))  \n",
       "13                       ((ISLAMIC, NNP), (LAW, NNP))  \n",
       "14                 ((Steinn, NNP), (Sigurdsson, NNP))  \n",
       "15                       ((Beam, NNP), (Jockey, NNP))  \n",
       "16  ((icsucieduincominggeodegif, NN), (icsucieduin...  \n",
       "17                   ((GENERAL, NNP), (UNIFIED, NNP))  \n",
       "18  ((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))  \n",
       "19                    ((Tourist, NNP), (Bureau, NNP))  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "#(a)\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_data = fetch_20newsgroups(subset='all', shuffle=True, categories=['alt.atheism','talk.religion.misc','comp.graphics','sci.space'])\n",
    "data = pd.Series(twenty_data.data).astype(str)\n",
    "\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or (ord(i)==32) or (ord(i)==10))\n",
    "clean_data = data.map(lambda x: _removeNonAscii(x))\n",
    "\n",
    "STOPWORDS_DICT = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n",
    "\n",
    "def get_language(text):\n",
    "    words = set(nltk.wordpunct_tokenize(text.lower()))\n",
    "    lang = max(((lang, len(words & stopwords)) for lang, stopwords in STOPWORDS_DICT.items()), key = lambda x: x[1])[0]\n",
    "    if lang == 'english':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "english_data=clean_data[clean_data.apply(get_language)]\n",
    "english_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "token_data = []\n",
    "for i in range(len(english_data)):\n",
    "    if english_data[i]:\n",
    "        token_data.append(word_tokenize(english_data[i]))\n",
    "\n",
    "POS_data = []\n",
    "for i in range(len(token_data)):\n",
    "    temp = nltk.pos_tag(token_data[i])\n",
    "    POS_data.extend(temp)\n",
    "\n",
    "        \n",
    "#(b)\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(POS_data)\n",
    "\n",
    "bigram_freq = finder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    if ngram[0][1] in acceptable_types and ngram[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Freuency filter\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "freq_bi = filtered_bi[:20].bigram.values\n",
    "\n",
    "#PMI\n",
    "finder.apply_freq_filter(20)\n",
    "bigramPMITable = pd.DataFrame(list(finder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "filteredPMI_bi = bigramPMITable[bigramPMITable.bigram.map(lambda x: rightTypes(x))]\n",
    "pmi_bi = filteredPMI_bi[:20].bigram.values\n",
    "\n",
    "#T-test\n",
    "bigramTtable = pd.DataFrame(list(finder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n",
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n",
    "t_bi = filteredT_bi[:20].bigram.values\n",
    "\n",
    "#Chi-squared\n",
    "bigramChiTable = pd.DataFrame(list(finder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
    "filteredChi_bi = bigramChiTable[bigramChiTable.bigram.map(lambda x: rightTypes(x))]\n",
    "chi_bi = filteredChi_bi[:20].bigram.values\n",
    "\n",
    "#DataFrame showing top20 results for each method\n",
    "df = pd.DataFrame({'Freq': freq_bi, 'PMI': pmi_bi, 't-test': t_bi, 'Chi-squared': chi_bi})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) There is quite a bit of overlap between the Frequency Filter method and the t-test method, so much so that both methods result in the same top 20 with 4 of them arranged differently. Meanwhile for chi-squared test and PMI methods there is a bit of overlap between but not nearly as much as between the other two.  \n",
    "\n",
    "I think that the union of the results would only make sense if there was a lot more filtering for actual words and if the duplicates were dropped.  \n",
    "    \n",
    "It is also apparent that the PMI and chi-squared methods are more likely to return collocations that aren't true English words, likely because those pairs of words only appear a few times (or even only once) together and never apart as is common with names of people, businesses, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#(a)\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Because I have already removed numbers non-letter characters I will just remove stopwords and stem the words now\n",
    "ps = PorterStemmer()\n",
    "stemmed = []\n",
    "for i in range(len(token_data)):\n",
    "    temp = []\n",
    "    for w in token_data[i]:\n",
    "        if w not in en_stopwords:\n",
    "            temp.append(ps.stem(w).lower())\n",
    "    stemmed.append(temp)\n",
    "\n",
    "#(b)\n",
    "#making a callable tokenizer function to bypass the Tfidfvectorizer's tokenization because my data is already tokenized\n",
    "def tokenize(text):\n",
    "    return text\n",
    "vect = TfidfVectorizer(tokenizer=tokenize, lowercase=False)\n",
    "X = vect.fit_transform(stemmed)\n",
    "\n",
    "#(c)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "#I have found that the document at index 87 was deleted because it wasn't in English\n",
    "target = twenty_data.target[np.arange(len(twenty_data.target))!=87]\n",
    "\n",
    "print(X)\n",
    "print(len(target))\n",
    "X_train, X_test, y_train, y_test = tts(X, target, test_size=0.3)\n",
    "\n",
    "svm = SVC(gamma='scale', kernel='rbf').fit(X_train, y_train)\n",
    "pred_svm = svm.predict(X_test)\n",
    "matrix_svm = confusion_matrix(y_test, pred_svm)\n",
    "print(\"SVM confusion matrix:\\n\", matrix_svm)\n",
    "\n",
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "pred_nb = nb.predict(X_test)\n",
    "matrix_nb = confusion_matrix(y_test, pred_nb)\n",
    "print(\"NB confusion matrix:\\n\", matrix_nb)\n",
    "\n",
    "svm_score = accuracy_score(y_test, pred_svm)\n",
    "nb_score = accuracy_score(y_test, pred_nb)\n",
    "print(\"\\nSVM score:\",svm_score,\"  NB score:\",nb_score)\n",
    "\n",
    "svm2 = SVC(gamma='scale', kernel='linear').fit(X_train, y_train)\n",
    "pred_svm2 = svm2.predict(X_test)\n",
    "svm_score2 = accuracy_score(y_test, pred_svm2)\n",
    "print(\"\\nkernel scores:\\nrbf:\",svm_score, \"linear:\",svm_score2)\n",
    "print(\"Yes the different kernels do affect the accuracy, especially between the linear (highest score) and rbf (default)\")\n",
    "\n",
    "\n",
    "#(d)\n",
    "tokens = []\n",
    "for i in range(len(data)):\n",
    "    tokens.append(word_tokenize(data[i]))\n",
    "\n",
    "POS = []\n",
    "for i in range(len(tokens)):\n",
    "    temp = nltk.pos_tag(tokens[i])\n",
    "    POS.extend(temp)\n",
    "    \n",
    "def correctTypes(text):\n",
    "    if '-pron-' in text or '' in text or ' 'in text or 't' in text:\n",
    "        return False\n",
    "    for word in text:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    if text[1] in acceptable_types:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "POS = pd.Series(POS)\n",
    "POS = POS.map(lambda x: _removeNonAscii(x))\n",
    "POS = POS[POS.map(lambda x: correctTypes(x))]\n",
    "POS = POS.tolist()\n",
    "    \n",
    "stems = []\n",
    "for i in range(len(POS)):\n",
    "    temp = []\n",
    "    for w in POS[i]:\n",
    "        if w not in en_stopwords:\n",
    "            temp.append(ps.stem(w).lower())\n",
    "    stems.append(temp)\n",
    "\n",
    "print(stems)\n",
    "X2 = vect.fit_transform(stems)\n",
    "\n",
    "#Repeat of question 'c'\n",
    "X_train2, X_test2, y_train2, y_test2 = tts(X2, target, test_size=0.3)\n",
    "\n",
    "svm_2 = SVC(gamma='scale', kernel='rbf').fit(X_train2, y_train2)\n",
    "pred_svm_2 = svm_2.predict(X_test2)\n",
    "matrix_svm_2 = confusion_matrix(y_test2, pred_svm_2)\n",
    "print(\"SVM confusion matrix:\\n\", matrix_svm_2)\n",
    "\n",
    "nb_2 = MultinomialNB().fit(X_train2, y_train2)\n",
    "pred_nb_2 = nb_2.predict(X_test2)\n",
    "matrix_nb_2 = confusion_matrix(y_test2, pred_nb_2)\n",
    "print(\"NB confusion matrix:\\n\", matrix_nb)\n",
    "\n",
    "svm_score_2 = accuracy_score(y_test2, pred_svm_2)\n",
    "nb_score_2 = accuracy_score(y_test2, pred_nb_2)\n",
    "print(\"\\nSVM score:\",svm_score_2,\"  NB score:\",nb_score_2)\n",
    "\n",
    "#The accuracy for the text classification is much higher when only the nouns are being used\n",
    "print(\"\\n# of words with nouns:\", len(POS_data), \"   # of words without nouns:\", len(POS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
